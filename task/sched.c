/*

Copyright 2024, 2025 Amar Djulovic <aaamargml@gmail.com>

This file is part of FloppaOS.

FloppaOS is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

FloppaOS is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with FloppaOS. If not, see <https://www.gnu.org/licenses/>.

*/

#include "sched.h"
#include "thread.h"
#include "../mem/alloc.h"
#include "../mem/pmm.h"
#include "../mem/paging.h"
#include "../mem/vmm.h"
#include "../mem/utils.h"
#include "../lib/logging.h"
#include "../lib/str.h"
#include "../interrupts/interrupts.h"
#include "../task/sync/spinlock.h"
#include "../drivers/time/floptime.h"
#include <stdint.h>
#include <stddef.h>
#include <stdbool.h>

uint64_t sched_ticks_counter;

typedef struct signal {
    atomic_int state;
} signal_t;

typedef struct reaper_descriptor {
    thread_list_t dead_threads;
    spinlock_t lock;
    int running;
    signal_t wake_signal;
    thread_t* reaper_thread;
} reaper_descriptor_t;

static reaper_descriptor_t thread_reaper;

static void idle_thread_loop() {
    for (;;) {
    }
}

void sched_wake_reaper(void) {
    if (!sched.reaper_thread)
        return;

    thread_t* reaper = sched.reaper_thread;

    atomic_store(&thread_reaper.wake_signal.state, 1);

    if (reaper->thread_state == THREAD_RUNNING || reaper->thread_state == THREAD_READY) {
        return;
    }

    if (reaper->thread_state == THREAD_SLEEPING) {
        spinlock(&sched.sleep_queue->lock);

        thread_t* prev = NULL;
        thread_t* curr = sched.sleep_queue->head;

        while (curr) {
            if (curr == reaper) {
                if (prev) {
                    prev->next = curr->next;
                } else {
                    sched.sleep_queue->head = curr->next;
                }

                if (curr == sched.sleep_queue->tail) {
                    sched.sleep_queue->tail = prev;
                }

                sched.sleep_queue->count--;
                curr->next = NULL;
                break;
            }
            prev = curr;
            curr = curr->next;
        }

        spinlock_unlock(&sched.sleep_queue->lock, true);

        // requeue in ready list
        reaper->thread_state = THREAD_READY;
        sched_enqueue(sched.ready_queue, reaper);
    }
}

static inline void signal_wait(signal_t* s) {
    while (atomic_load(&s->state) == 0) {
        sched_yield();
    }
    atomic_store(&s->state, 0);
}

static inline void signal_init(signal_t* s) {
    atomic_store(&s->state, 0);
}

static inline void signal_send(signal_t* s) {
    atomic_store(&s->state, 1);
    sched_wake_reaper();
}

static void reaper_thread_entry(void) {
    log("reaper: thread started", GREEN);

    while (thread_reaper.running) {
        signal_wait(&thread_reaper.wake_signal);

        while (1) {
            spinlock(&thread_reaper.lock);
            thread_t* dead = sched_dequeue(&thread_reaper.dead_threads);
            spinlock_unlock(&thread_reaper.lock, true);

            if (!dead)
                break;

            // Cleanup
            if (dead->kernel_stack)
                kfree(dead->kernel_stack, 4096);

            if (dead->user && dead->process)
                sched_remove(dead->process->threads, dead);

            kfree(dead, sizeof(thread_t));
        }

        sched_yield();
    }

    log("reaper: exiting", YELLOW);
}

static thread_t*
sched_internal_init_thread(void (*entry)(void), unsigned int priority, char* name, int user, process_t* process);

void reaper_init(void) {
    flop_memset(&thread_reaper, 0, sizeof(thread_reaper));
    spinlock_init(&thread_reaper.lock);
    signal_init(&thread_reaper.wake_signal);
    thread_reaper.running = 1;

    thread_reaper.reaper_thread = sched_internal_init_thread(reaper_thread_entry, 1, "reaper", 0, NULL);

    sched_enqueue(sched.ready_queue, thread_reaper.reaper_thread);
    sched.reaper_thread = thread_reaper.reaper_thread;

    log("reaper: initialized", GREEN);
}

static void stealer_thread_entry() {}

extern void context_switch(cpu_ctx_t* old, cpu_ctx_t* new);
extern void usermode_entry_routine(uint32_t stack, uint32_t ip);
scheduler_t sched;
#define USER_STACK_TOP 0xC0000000
#define USER_STACK_SIZE 0x1000

static uintptr_t sched_internal_alloc_user_stack(process_t* process, uint32_t stack_index) {
    uintptr_t user_stack_top = USER_STACK_TOP - (stack_index * USER_STACK_SIZE);
    uintptr_t phys = (uintptr_t) pmm_alloc_page();
    if (!phys) {
        return 0;
    }
    if (vmm_map(process->region, user_stack_top - USER_STACK_SIZE, phys, PAGE_RW | PAGE_USER) < 0) {
        pmm_free_page((void*) phys);
        return 0;
    }

    return user_stack_top;
}

static void sched_internal_setup_thread_stack(thread_t* thread, void (*entry)(void), uintptr_t user_stack_top) {
    uint32_t* kstack = (uint32_t*) ((uintptr_t) thread->kernel_stack + 4096);

    *--kstack = (uint32_t) entry;
    *--kstack = (uint32_t) user_stack_top;

    thread->context.eip = (uint32_t) usermode_entry_routine;
    thread->kernel_stack = kstack;
}

static inline uint32_t sched_internal_fetch_next_stack_index(process_t* process) {
    return process->threads ? process->threads->count : 0;
}

int sched_spinlocks_init(void) {
    static spinlock_t ready_queue_spinlock_initializer = SPINLOCK_INIT;
    static spinlock_t sleep_queue_spinlock_initializer = SPINLOCK_INIT;
    static spinlock_t kernel_threads_spinlock_initializer = SPINLOCK_INIT;
    static spinlock_t user_threads_spinlock_initializer = SPINLOCK_INIT;
    spinlock_init(&sched.ready_queue->lock);
    spinlock_init(&sched.sleep_queue->lock);
    spinlock_init(&sched.kernel_threads->lock);
    spinlock_init(&sched.user_threads->lock);
    return 0;
}

int sched_scheduler_lists_init(void) {
    sched.ready_queue = kmalloc(sizeof(thread_list_t));
    if (!sched.ready_queue)
        return -1;
    sched.sleep_queue = kmalloc(sizeof(thread_list_t));
    if (!sched.sleep_queue)
        return -1;
    sched.kernel_threads = kmalloc(sizeof(thread_list_t));
    if (!sched.kernel_threads)
        return -1;
    sched.user_threads = kmalloc(sizeof(thread_list_t));
    if (!sched.user_threads)
        return -1;

    if (sched_spinlocks_init() < 0) {
        return -1;
    }
    flop_memset(sched.ready_queue, 0, sizeof(thread_list_t));
    flop_memset(sched.sleep_queue, 0, sizeof(thread_list_t));
    flop_memset(sched.kernel_threads, 0, sizeof(thread_list_t));
    flop_memset(sched.user_threads, 0, sizeof(thread_list_t));
    return 0;
}

int sched_assign_list_names(void) {
    sched.ready_queue->name = "ready_queue";
    sched.sleep_queue->name = "sleep_queue";
    sched.kernel_threads->name = "kernel_threads";
    sched.user_threads->name = "user_threads";
    return 0;
}

int sched_init_kernel_worker_pool(void);

// set up thread lists
// init list spinlocks
// and create reaper and idle threads
void sched_init(void) {
    if (sched_scheduler_lists_init() < 0) {
        log("sched: failed to init scheduler lists\n", RED);
        return;
    }

    if (sched_assign_list_names() < 0) {
        log("sched: failed to init scheduler list names\n", RED);
        return;
    }

    if (sched_init_kernel_worker_pool() < 0) {
        log("sched: failed to init kernel worker pool\n", RED);
        return;
    }

    sched.stealer_thread = NULL;
    sched.next_tid = 0;
    reaper_init();
    sched_thread_list_add(sched.idle_thread, sched.ready_queue);

    log("sched init - ok", GREEN);
}

// add thread to the end of a thread queue
// uses the FIFO method
void sched_enqueue(thread_list_t* list, thread_t* thread) {
    if (!list || !thread) {
        return;
    }

    // we must lock when accessesing a thread list
    // this can lead to a race condition if many cores access it at once
    // this isnt a huge concern for now but it does disable interrupts
    // which is important for us here.
    spinlock(&list->lock);

    thread->next = NULL;

    if (!list->head) {
        list->head = thread;
        list->tail = thread;
    } else {
        list->tail->next = thread;
        list->tail = thread;
    }

    // atomic addition for increasing list->count
    atomic_fetch_add_explicit((atomic_uint*) &list->count, 1, memory_order_release);

    spinlock_unlock(&list->lock, true);
}

// remove head of a thread queue
thread_t* sched_dequeue(thread_list_t* list) {
    if (!list) {
        return NULL;
    }

    // we must lock when accessesing a thread list
    spinlock(&list->lock);

    thread_t* thread = list->head;
    if (!thread) {
        spinlock_unlock(&list->lock, true);
        return NULL;
    }

    list->head = thread->next;

    // if the list is empty we set list->tail to NULL which
    // signifies the list is empty.
    if (!list->head) {
        list->tail = NULL;
    }

    atomic_fetch_sub_explicit((atomic_uint*) &list->count, 1, memory_order_release);

    thread->next = NULL;

    spinlock_unlock(&list->lock, true);

    return thread;
}

// remove arbritrary thread_t* "target" from queue.
// slower than dequeue because its (o(n))
// its probably better to use dequeue()
thread_t* sched_remove(thread_list_t* list, thread_t* target) {
    if (!list || !target) {
        return NULL;
    }
    spinlock(&list->lock);
    thread_t* prev = NULL;
    thread_t* curr = list->head;
    while (curr) {
        if (curr == target) {
            if (prev) {
                prev->next = curr->next;
            } else {
                list->head = curr->next;
            }

            if (curr == list->tail) {
                list->tail = prev;
            }

            list->count--;
            curr->next = NULL;

            spinlock_unlock(&list->lock, true);
            return curr;
        }
        prev = curr;
        curr = curr->next;
    }
    spinlock_unlock(&list->lock, true);
    return NULL;
}

static thread_t*
sched_internal_init_thread(void (*entry)(void), unsigned priority, char* name, int user, process_t* process) {
    // todo: create slab cache for thread structs
    thread_t* this_thread = kmalloc(sizeof(thread_t));
    this_thread->next = NULL;
    this_thread->previous = NULL;
    this_thread->kernel_stack = (void*) kmalloc(4096);
    if (!this_thread->kernel_stack) {
        // stack allocation failed
        return NULL;
    }

    // effective priority starts the same as base priority
    // it will be boosted if the thread is starved
    // the scheduler will always use the effective priority
    // the base priority is simply the priority assigned upon thread creation
    this_thread->priority.base = priority;
    this_thread->priority.effective = priority;
    cpu_ctx_t this_thread_context = {.edi = 0,
                                     .esi = 0,
                                     .ebx = 0,
                                     .ebp = 0,
                                     // user threads will set eip to the user mode entry routine, use wrapper.
                                     .eip = (uint32_t) entry};

    this_thread->context = this_thread_context;

    if (user) {
        this_thread->user = 1;
        if (process == NULL) {
            // user threads require a process
            return NULL;
        }
        this_thread->process = process;
    } else {
        this_thread->user = 0;
        if (process != NULL) {
            return NULL;
            // kernel threads cannot have a process
            // that's actually the only thing that makes them kernel threads.
        }
        this_thread->process = NULL;
        // this must remain null.
    }

    this_thread->id = sched.next_tid;
    sched.next_tid++;

    this_thread->name = name;
    this_thread->uptime = 0;
    this_thread->time_since_last_run = 0;
    this_thread->time_slice = priority * 2;

    return this_thread;
}

thread_t* sched_create_user_thread(void (*entry)(void), unsigned priority, char* name, process_t* process) {
    if (!process)
        return NULL;

    thread_t* new_thread = sched_internal_init_thread((void*) usermode_entry_routine, priority, name, 1, process);

    if (!new_thread) {
        return NULL;
    }

    uint32_t stack_index = sched_internal_fetch_next_stack_index(process);
    uintptr_t user_stack_top = sched_internal_alloc_user_stack(process, stack_index);

    if (!user_stack_top) {
        kfree(new_thread, sizeof(thread_t));
        return NULL;
    }

    sched_internal_setup_thread_stack(new_thread, entry, user_stack_top);

    // in addition to adding this thread to the thread queue in enqueue()
    // we need to add it to the linked list of threads for the process.
    // NOTE: this is not a thread queue. it is a list of threads for each process.
    sched_thread_list_add(new_thread, process->threads);

    // we must also add it to the linked list of user threads
    sched_thread_list_add(new_thread, sched.user_threads);
    log("sched: user thread created", GREEN);
    return new_thread;
}

void reaper_enqueue(thread_t* thread) {
    if (!thread)
        return;

    thread->thread_state = THREAD_DEAD;

    spinlock(&thread_reaper.lock);
    sched_enqueue(&thread_reaper.dead_threads, thread);
    spinlock_unlock(&thread_reaper.lock, true);
}

void sched_thread_list_add(thread_t* thread, thread_list_t* list) {
    if (!thread || !list)
        return;

    spinlock(&list->lock);

    thread->next = NULL;

    if (!list->head) {
        list->head = thread;
        list->tail = thread;
    } else {
        list->tail->next = thread;
        list->tail = thread;
    }

    list->count++;

    spinlock_unlock(&list->lock, true);
}

thread_t* sched_create_kernel_thread(void (*entry)(void), unsigned priority, char* name) {
    thread_t* new_thread = sched_internal_init_thread((void*) entry, priority, name, 0, NULL);
    log("sched: kernel thread created", GREEN);
    sched_thread_list_add(new_thread, sched.kernel_threads);
    return new_thread;
}

thread_t* current_thread;

// our context switch takes our cpu context struct which
// contains edi, esi, ebx, ebp, eip
// we do not need to save esp because we have it in thread->kernel_stack
extern void context_switch(cpu_ctx_t* old, cpu_ctx_t* new);
extern void usermode_entry_routine(uint32_t sp, uint32_t ip);

void sched_boost_starved_threads(thread_list_t* list) {
    spinlock(&list->lock);

    for (thread_t* t = list->head; t; t = t->next) {
        t->time_since_last_run++;
        if (t->time_since_last_run > STARVATION_THRESHOLD && t->priority.effective < MAX_PRIORITY) {
            t->priority.effective += BOOST_AMOUNT;
        }
    }

    spinlock_unlock(&list->lock, true);
}

static thread_t* sched_select_by_time_slice(thread_list_t* list) {
    if (!list || !list->head)
        return NULL;

    thread_t* iter = list->head;
    thread_t* best = iter;
    thread_t* best_prev = NULL;
    thread_t* prev = NULL;

    while (iter) {
        if (iter->priority.base > best->priority.base) {
            best_prev = prev;
            best = iter;
        }
        prev = iter;
        iter = iter->next;
    }

    if (!best)
        return NULL;

    if (best_prev)
        best_prev->next = best->next;
    else
        list->head = best->next;

    if (best == list->tail)
        list->tail = best_prev;

    if (list->count > 0)
        list->count--;

    best->next = NULL;
    best->time_slice = best->priority.base ? best->priority.base : 1;
    return best;
}

void sched_schedule(void) {
    sched_boost_starved_threads(sched.ready_queue);

    spinlock(&sched.ready_queue->lock);
    thread_t* next = sched_select_by_time_slice(sched.ready_queue);
    spinlock_unlock(&sched.ready_queue->lock, true);

    if (!next) {
        next = sched.idle_thread;
        next->time_slice = next->priority.base ? next->priority.base : 1;
    }

    if (next == current_thread)
        return;

    next->time_since_last_run = 0;

    thread_t* prev = current_thread;
    current_thread = next;

    context_switch(&prev->context, &next->context);
}

thread_t* sched_current_thread(void) {
    return current_thread;
}

void sched_thread_exit(void) {
    thread_t* current = sched_current_thread();
    reaper_enqueue(current);
    sched_yield();
}

void sched_yield(void) {
    if (!current_thread) {
        return;
    }
    if (current_thread != sched.idle_thread) {
        sched_enqueue(sched.ready_queue, current_thread);
    }

    sched_schedule();
}

void sched_thread_sleep(uint32_t ms) {
    thread_t* current = sched_current_thread();
    if (!current || ms == 0)
        return;

    current->wake_time = sched_ticks_counter + (uint64_t) ms;
    current->thread_state = THREAD_SLEEPING;

    sched_enqueue(sched.sleep_queue, current);
    sched_yield();
}

void sched_tick(void) {
    sched_ticks_counter++;

    // wake sleeping threads
    spinlock(&sched.sleep_queue->lock);
    thread_t* prev = NULL;
    thread_t* curr = sched.sleep_queue->head;

    while (curr) {
        thread_t* next = curr->next;

        if (curr->wake_time <= sched_ticks_counter) {
            if (prev)
                prev->next = next;
            else
                sched.sleep_queue->head = next;

            if (curr == sched.sleep_queue->tail)
                sched.sleep_queue->tail = prev;

            sched.sleep_queue->count--;

            curr->next = NULL;
            curr->thread_state = THREAD_READY;
            sched_enqueue(sched.ready_queue, curr);
        } else {
            prev = curr;
        }

        curr = next;
    }

    spinlock_unlock(&sched.sleep_queue->lock, true);
}

typedef struct worker_thread {
    thread_t* thread;
    void (*entry)(void*);
    void* arg;

} worker_thread;

typedef struct worker_pool_descriptor {
    worker_thread** pool;
    size_t count;
    size_t min_count;
    size_t grow_by;
    void (*entry)(void*);
    void** args;
    unsigned priority;
    char* name;
} worker_pool_descriptor_t;

static worker_thread* sched_internal_init_worker(void (*entry)(void*), void* arg, unsigned priority, char* name) {
    worker_thread* worker = kmalloc(sizeof(worker_thread));
    if (!worker) {
        return NULL;
    }

    thread_t* thread = sched_internal_init_thread((void*) entry, priority, name, 0, NULL);
    if (!thread) {
        kfree(worker, sizeof(worker_thread));
        return NULL;
    }

    worker->thread = thread;
    worker->entry = entry;
    worker->arg = arg;
    return worker;
}

static worker_thread* sched_create_worker_thread(void (*entry)(void*), void* arg, unsigned priority, char* name) {
    worker_thread* worker = sched_internal_init_worker(entry, arg, priority, name);
    if (!worker) {
        return NULL;
    }
    sched_thread_list_add(worker->thread, sched.kernel_threads);
    return worker;
}

int sched_create_worker_pool(worker_pool_descriptor_t* desc, size_t count) {
    if (!desc || count == 0 || !desc->entry) {
        return -1;
    }

    worker_thread** pool = kmalloc(sizeof(worker_thread*) * count);
    if (!pool) {
        return -1;
    }

    size_t created = 0;
    for (size_t i = 0; i < count; ++i) {
        void* arg = desc->args ? desc->args[i] : NULL;
        worker_thread* w = sched_create_worker_thread(desc->entry, arg, desc->priority, desc->name);
        if (!w) {
            break;
        }

        sched_enqueue(sched.ready_queue, w->thread);
        pool[created++] = w;
    }

    if (created < count) {
        for (size_t j = 0; j < created; ++j) {
            worker_thread* cw = pool[j];
            if (!cw) {
                continue;
            }
            sched_remove(sched.kernel_threads, cw->thread);
            if (cw->thread->kernel_stack) {
                kfree(cw->thread->kernel_stack, 4096);
            }
            kfree(cw->thread, sizeof(thread_t));
            kfree(cw, sizeof(worker_thread));
        }
        kfree(pool, sizeof(worker_thread*) * count);
        return -1;
    }

    desc->pool = pool;
    desc->count = count;
    return 0;
}

int sched_expand_worker_pool(worker_pool_descriptor_t* desc, size_t target_count) {
    if (!desc || target_count == 0) {
        return -1;
    }

    if (desc->count >= target_count) {
        return 0;
    }

    worker_thread** new_pool = kmalloc(sizeof(worker_thread*) * target_count);
    if (!new_pool) {
        return -1;
    }

    for (size_t i = 0; i < desc->count; ++i) {
        new_pool[i] = desc->pool[i];
    }

    size_t created = 0;
    for (size_t i = desc->count; i < target_count; ++i) {
        void* arg = desc->args ? desc->args[i] : NULL;
        worker_thread* w = sched_create_worker_thread(desc->entry, arg, desc->priority, desc->name);
        if (!w) {
            break;
        }
        sched_enqueue(sched.ready_queue, w->thread);
        new_pool[i] = w;
        created++;
    }

    if (desc->pool) {
        kfree(desc->pool, sizeof(worker_thread*) * desc->count);
    }

    desc->pool = new_pool;
    desc->count = desc->count + created;
    return (desc->count < target_count) ? -1 : 0;
}

static void worker_pool_manager_entry(void* _arg) {
    worker_pool_descriptor_t* desc = (worker_pool_descriptor_t*) _arg;
    if (!desc) {
        return;
    }

    for (;;) {
        if (desc->count < desc->min_count) {
            size_t target = desc->count + (desc->grow_by ? desc->grow_by : desc->min_count - desc->count);
            if (target < desc->min_count) {
                target = desc->min_count;
            }
            sched_expand_worker_pool(desc, target);
        }
        sched_yield();
    }
}

int sched_start_worker_pool_manager(worker_pool_descriptor_t* desc) {
    if (!desc || !desc->entry || desc->min_count == 0) {
        return -1;
    }

    worker_thread* mgr = sched_create_worker_thread(worker_pool_manager_entry, desc, desc->priority, "worker_pool_mgr");
    if (!mgr) {
        return -1;
    }

    sched_enqueue(sched.ready_queue, mgr->thread);
    return 0;
}

int sched_remove_worker_pool(worker_pool_descriptor_t* desc) {
    if (!desc || !desc->pool) {
        return -1;
    }

    for (size_t i = 0; i < desc->count; ++i) {
        worker_thread* w = desc->pool[i];
        if (!w) {
            continue;
        }
        sched_remove(sched.kernel_threads, w->thread);
        if (w->thread->kernel_stack) {
            kfree(w->thread->kernel_stack, 4096);
        }
        kfree(w->thread, sizeof(thread_t));
        kfree(w, sizeof(worker_thread));
    }

    kfree(desc->pool, sizeof(worker_thread*) * desc->count);
    desc->pool = NULL;
    desc->count = 0;
    return 0;
}

int sched_copy_worker_pool(worker_pool_descriptor_t* dest, const worker_pool_descriptor_t* src) {
    if (!dest || !src || !src->pool || src->count == 0) {
        return -1;
    }

    worker_thread** new_pool = kmalloc(sizeof(worker_thread*) * src->count);
    if (!new_pool) {
        return -1;
    }

    size_t copied = 0;
    for (size_t i = 0; i < src->count; ++i) {
        worker_thread* sw = src->pool[i];
        if (!sw) {
            new_pool[i] = NULL;
            continue;
        }

        worker_thread* dw = sched_internal_init_worker(sw->entry, sw->arg, sw->thread->priority.base, sw->thread->name);
        if (!dw) {
            break;
        }

        sched_thread_list_add(dw->thread, sched.kernel_threads);
        new_pool[i] = dw;
        copied++;
    }

    if (copied < src->count) {
        for (size_t j = 0; j < copied; ++j) {
            worker_thread* w = new_pool[j];
            if (!w) {
                continue;
            }
            sched_remove(sched.kernel_threads, w->thread);
            if (w->thread->kernel_stack) {
                kfree(w->thread->kernel_stack, 4096);
            }
            kfree(w->thread, sizeof(thread_t));
            kfree(w, sizeof(worker_thread));
        }
        kfree(new_pool, sizeof(worker_thread*) * src->count);
        return -1;
    }

    if (dest->pool) {
        kfree(dest->pool, sizeof(worker_thread*) * dest->count);
    }

    dest->pool = new_pool;
    dest->count = src->count;
    dest->entry = src->entry;
    dest->args = src->args;
    dest->priority = src->priority;
    dest->name = src->name;

    return 0;
}

int sched_copy_selected_workers(worker_pool_descriptor_t* dest,
                                const worker_pool_descriptor_t* src,
                                const size_t* indices,
                                size_t indices_count) {
    if (!dest || !src || !src->pool || !indices || indices_count == 0) {
        return -1;
    }

    worker_thread** new_pool = kmalloc(sizeof(worker_thread*) * indices_count);
    if (!new_pool) {
        return -1;
    }

    size_t copied = 0;
    for (size_t i = 0; i < indices_count; ++i) {
        size_t idx = indices[i];
        if (idx >= src->count) {
            new_pool[i] = NULL;
            continue;
        }

        worker_thread* sw = src->pool[idx];
        if (!sw) {
            new_pool[i] = NULL;
            continue;
        }

        worker_thread* dw = sched_internal_init_worker(sw->entry, sw->arg, sw->thread->priority.base, sw->thread->name);
        if (!dw) {
            break;
        }

        sched_thread_list_add(dw->thread, sched.kernel_threads);
        new_pool[i] = dw;
        copied++;
    }

    if (copied < indices_count) {
        for (size_t j = 0; j < copied; ++j) {
            worker_thread* w = new_pool[j];
            if (!w) {
                continue;
            }
            sched_remove(sched.kernel_threads, w->thread);
            if (w->thread->kernel_stack) {
                kfree(w->thread->kernel_stack, 4096);
            }
            kfree(w->thread, sizeof(thread_t));
            kfree(w, sizeof(worker_thread));
        }
        kfree(new_pool, sizeof(worker_thread*) * indices_count);
        return -1;
    }

    if (dest->pool) {
        kfree(dest->pool, sizeof(worker_thread*) * dest->count);
    }

    dest->pool = new_pool;
    dest->count = indices_count;
    dest->entry = src->entry;
    dest->args = src->args;
    dest->priority = src->priority;
    dest->name = src->name;

    return 0;
}

int sched_delete_specified_workers_from_pool(worker_pool_descriptor_t* desc,
                                             const size_t* indices,
                                             size_t indices_count) {
    if (!desc || !desc->pool || !indices || indices_count == 0) {
        return -1;
    }

    for (size_t i = 0; i < indices_count; ++i) {
        size_t idx = indices[i];
        if (idx >= desc->count) {
            continue;
        }

        worker_thread* w = desc->pool[idx];
        if (!w) {
            continue;
        }

        sched_remove(sched.kernel_threads, w->thread);
        if (w->thread->kernel_stack) {
            kfree(w->thread->kernel_stack, 4096);
        }
        kfree(w->thread, sizeof(thread_t));
        kfree(w, sizeof(worker_thread));

        desc->pool[idx] = NULL;
    }

    return 0;
}

int sched_add_workers_to_pool(worker_pool_descriptor_t* desc, worker_thread** new_workers, size_t new_count) {
    if (!desc || !new_workers || new_count == 0) {
        return -1;
    }

    size_t target_count = desc->count + new_count;
    worker_thread** new_pool = kmalloc(sizeof(worker_thread*) * target_count);
    if (!new_pool) {
        return -1;
    }

    for (size_t i = 0; i < desc->count; ++i) {
        new_pool[i] = desc->pool[i];
    }

    for (size_t i = 0; i < new_count; ++i) {
        worker_thread* w = new_workers[i];
        if (!w) {
            new_pool[desc->count + i] = NULL;
            continue;
        }
        sched_thread_list_add(w->thread, sched.kernel_threads);
        new_pool[desc->count + i] = w;
    }

    if (desc->pool) {
        kfree(desc->pool, sizeof(worker_thread*) * desc->count);
    }

    desc->pool = new_pool;
    desc->count = target_count;
    return 0;
}

void sched_worker_thread_print(void) {
    log("Kernel Worker Threads:\n", YELLOW);
    spinlock(&sched.kernel_threads->lock);
    for (thread_t* t = sched.kernel_threads->head; t; t = t->next) {
        char buffer[128];
        flopsnprintf(buffer, sizeof(buffer), " - %s (priority: %u)\n", t->name, t->priority.base);
        log(buffer, YELLOW);
    }
    spinlock_unlock(&sched.kernel_threads->lock, true);

    log("User Threads:\n", YELLOW);
    spinlock(&sched.user_threads->lock);
    for (thread_t* t = sched.user_threads->head; t; t = t->next) {
        char buffer[128];
        flopsnprintf(buffer, sizeof(buffer), " - %s (priority: %u)\n", t->name, t->priority.base);
        log(buffer, YELLOW);
    }
    spinlock_unlock(&sched.user_threads->lock, true);
}

void sched_worker_thread_count_print(void) {
    char buffer[128];
    flopsnprintf(buffer,
                 sizeof(buffer),
                 "Kernel Worker Threads: %u\nUser Threads: %u\n",
                 sched.kernel_threads->count,
                 sched.user_threads->count);
    log(buffer, YELLOW);
}

void sched_worker_pool_print(worker_pool_descriptor_t* desc) {
    if (!desc || !desc->pool || desc->count == 0) {
        log("Worker pool is empty\n", YELLOW);
        return;
    }

    log("Worker Pool Descriptor Info:\n", CYAN);
    char header[256];
    flopsnprintf(header,
                 sizeof(header),
                 " - Name: %s\n - Priority: %u\n - Threads: %zu\n - Min: %zu\n - GrowBy: %zu\n\n",
                 desc->name ? desc->name : "(unnamed)",
                 desc->priority,
                 desc->count,
                 desc->min_count,
                 desc->grow_by);
    log(header, CYAN);

    log("Worker Pool Threads:\n", YELLOW);
    for (size_t i = 0; i < desc->count; ++i) {
        worker_thread* w = desc->pool[i];
        if (!w) {
            log(" - [NULL WORKER]\n", RED);
            continue;
        }

        char buffer[256];
        flopsnprintf(buffer,
                     sizeof(buffer),
                     " - [%zu] %s (priority: %u, thread=%p, arg=%p)\n",
                     i,
                     w->thread->name ? w->thread->name : "(unnamed)",
                     w->thread->priority.base,
                     w->thread,
                     w->arg);
        log(buffer, YELLOW);
    }
}

void sched_worker_pool_summary(worker_pool_descriptor_t* desc) {
    if (!desc) {
        log("sched: invalid worker pool descriptor\n", RED);
        return;
    }

    char buffer[256];
    flopsnprintf(buffer,
                 sizeof(buffer),
                 "Worker Pool Info:\n"
                 " - Name: %s\n"
                 " - Thread Count: %zu\n"
                 " - Min Count: %zu\n"
                 " - Grow By: %zu\n"
                 " - Priority: %u\n"
                 " - Entry: %p\n"
                 " - Args: %p\n",
                 desc->name ? desc->name : "(unnamed)",
                 desc->count,
                 desc->min_count,
                 desc->grow_by,
                 desc->priority,
                 desc->entry,
                 desc->args);
    log(buffer, CYAN);
}

void sched_all_pools_print(worker_pool_descriptor_t** pools, size_t pool_count) {
    if (!pools || pool_count == 0) {
        log("No worker pools defined.\n", YELLOW);
        return;
    }

    log("==== Worker Pools ====\n", GREEN);
    for (size_t i = 0; i < pool_count; ++i) {
        char buffer[128];
        flopsnprintf(buffer, sizeof(buffer), "[%zu] ", i);
        log(buffer, GREEN);
        sched_worker_pool_summary(pools[i]);
    }
}

int sched_init_kernel_worker_pool(void) {
    static worker_pool_descriptor_t kernel_worker_pool_desc = {
        .pool = NULL,
        .count = 0,
        .min_count = 4,
        .grow_by = 2,
        .entry = NULL,
        .args = NULL,
        .priority = 5,
        .name = "kernel_worker_thread",
    };

    kernel_worker_pool_desc.entry = NULL;

    if (sched_create_worker_pool(&kernel_worker_pool_desc, kernel_worker_pool_desc.min_count) < 0) {
        log("sched: failed to create kernel worker pool\n", RED);
        return -1;
    }

    if (sched_start_worker_pool_manager(&kernel_worker_pool_desc) < 0) {
        log("sched: failed to start kernel worker pool manager\n", RED);
        return -1;
    }

    log("sched: kernel worker pool initialized\n", GREEN);
    return 0;
}

int sched_create_process_worker_pool(process_t* process, worker_pool_descriptor_t* desc, size_t count) {
    if (!process || !desc || count == 0) {
        return -1;
    }

    if (sched_create_worker_pool(desc, count) < 0) {
        log("sched: failed to create process worker pool\n", RED);
        return -1;
    }

    log("sched: process worker pool created\n", GREEN);
    return 0;
}

int sched_destroy_process_worker_pool(worker_pool_descriptor_t* desc) {
    if (!desc) {
        return -1;
    }

    if (sched_remove_worker_pool(desc) < 0) {
        log("sched: failed to remove process worker pool\n", RED);
        return -1;
    }

    log("sched: process worker pool removed\n", GREEN);
    return 0;
}
